{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preprocessing and augmentation for CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # EfficientNet expects 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained EfficientNetB0 from torchvision\n",
    "model = models.efficientnet_b0(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classifier to fit the number of classes in CIFAR-10 (10 classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the GPU (if available)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training state\n",
    "def save_checkpoint(model, optimizer, epoch, batch_idx, file_path=\"checkpoint.pth\"):\n",
    "    state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx\n",
    "    }\n",
    "    torch.save(state, file_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}, batch {batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training state\n",
    "def load_checkpoint(model, optimizer, file_path=\"checkpoint.pth\"):\n",
    "    if os.path.exists(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        batch_idx = checkpoint['batch_idx']\n",
    "        print(f\"Checkpoint loaded: epoch {epoch}, batch {batch_idx}\")\n",
    "        return epoch, batch_idx\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_33784\\260820289.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded: epoch 0, batch 294\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint if it exists\n",
    "start_epoch, start_batch = load_checkpoint(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Checkpoint saved at epoch 0, batch 294\n",
      "Checkpoint saved at epoch 0, batch 295\n",
      "Checkpoint saved at epoch 0, batch 296\n",
      "Checkpoint saved at epoch 0, batch 297\n",
      "Checkpoint saved at epoch 0, batch 298\n",
      "Checkpoint saved at epoch 0, batch 299\n",
      "Batch [300/1563], Loss: 0.0083, Accuracy: 0.8802\n",
      "Checkpoint saved at epoch 0, batch 300\n",
      "Checkpoint saved at epoch 0, batch 301\n",
      "Checkpoint saved at epoch 0, batch 302\n",
      "Checkpoint saved at epoch 0, batch 303\n",
      "Checkpoint saved at epoch 0, batch 304\n",
      "Checkpoint saved at epoch 0, batch 305\n",
      "Checkpoint saved at epoch 0, batch 306\n",
      "Checkpoint saved at epoch 0, batch 307\n",
      "Checkpoint saved at epoch 0, batch 308\n",
      "Checkpoint saved at epoch 0, batch 309\n",
      "Checkpoint saved at epoch 0, batch 310\n",
      "Checkpoint saved at epoch 0, batch 311\n",
      "Checkpoint saved at epoch 0, batch 312\n",
      "Checkpoint saved at epoch 0, batch 313\n",
      "Checkpoint saved at epoch 0, batch 314\n",
      "Checkpoint saved at epoch 0, batch 315\n",
      "Checkpoint saved at epoch 0, batch 316\n",
      "Checkpoint saved at epoch 0, batch 317\n",
      "Checkpoint saved at epoch 0, batch 318\n",
      "Checkpoint saved at epoch 0, batch 319\n",
      "Checkpoint saved at epoch 0, batch 320\n",
      "Checkpoint saved at epoch 0, batch 321\n",
      "Checkpoint saved at epoch 0, batch 322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Accumulate metrics\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model and display progress for each batch\n",
    "num_epochs = 10\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Skip already completed batches if resuming\n",
    "        if epoch == start_epoch and batch_idx < start_batch:\n",
    "            continue\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Save checkpoint after each batch\n",
    "        save_checkpoint(model, optimizer, epoch, batch_idx)\n",
    "\n",
    "        # Print statistics every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Batch [{batch_idx + 1}/{len(train_loader)}], '\n",
    "                  f'Loss: {running_loss / (batch_idx + 1):.4f}, '\n",
    "                  f'Accuracy: {total_correct / total:.4f}')\n",
    "\n",
    "    # After each epoch, evaluate on test data\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total_test_correct = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test_correct += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = total_correct / total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = total_test_correct / total_test\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Save checkpoint at the end of the epoch\n",
    "    save_checkpoint(model, optimizer, epoch + 1, 0)  # Set batch_idx to 0 for next epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
