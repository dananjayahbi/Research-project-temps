{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = \"../DATA_PREPARE_ATT_03/Splitted_MyDataset02\"  # Root directory of the split dataset (train, val, test)\n",
    "model_save_path = \"efficientnet_b0_emotion_model.pth\"  # Path to save the trained model\n",
    "metadata_save_path = \"saved_metadata\"  # Path to save dataset metadata\n",
    "checkpoint_path = \"training_checkpoint.pth\"  # Path to save training checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "num_classes = 7\n",
    "patience = 5  # Early stopping patience\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Emotion categories\n",
    "emotion_classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "# Define focus weights for prioritized emotions (Anger, Sad, Happy, Neutral)\n",
    "focus_weights = [1.5 if emotion in [\"Anger\", \"Sad\", \"Happy\", \"Neutral\"] else 1.0 for emotion in emotion_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Metadata\n",
    "def save_dataset_metadata(train_dataset, val_dataset, test_dataset, class_weights, output_dir=\"saved_metadata\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"class_weights.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(class_weights, f)\n",
    "    dataset_info = {\n",
    "        \"train_indices\": train_dataset.samples,\n",
    "        \"val_indices\": val_dataset.samples,\n",
    "        \"test_indices\": test_dataset.samples,\n",
    "    }\n",
    "    with open(os.path.join(output_dir, \"dataset_info.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(dataset_info, f)\n",
    "    print(\"Dataset metadata and class weights saved.\")\n",
    "\n",
    "def load_dataset_metadata(output_dir=\"saved_metadata\"):\n",
    "    with open(os.path.join(output_dir, \"class_weights.pkl\"), \"rb\") as f:\n",
    "        class_weights = pickle.load(f)\n",
    "    with open(os.path.join(output_dir, \"dataset_info.pkl\"), \"rb\") as f:\n",
    "        dataset_info = pickle.load(f)\n",
    "    print(\"Dataset metadata and class weights loaded.\")\n",
    "    return class_weights, dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Missing Function\n",
    "def get_saved_data_loaders(data_dir, dataset_info, class_weights, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform)\n",
    "\n",
    "    train_dataset.samples = dataset_info[\"train_indices\"]\n",
    "    val_dataset.samples = dataset_info[\"val_indices\"]\n",
    "    test_dataset.samples = dataset_info[\"test_indices\"]\n",
    "\n",
    "    sample_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded successfully.\n",
      "Calculating class counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting samples per class: 100%|██████████| 98902/98902 [43:40<00:00, 37.74it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset metadata and class weights saved.\n",
      "Creating weighted sampler...\n",
      "Creating data loaders...\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Workflow\n",
    "def prepare_data_loaders(data_dir, batch_size, focus_weights, emotion_classes, metadata_save_path):\n",
    "    if os.path.exists(metadata_save_path):\n",
    "        print(\"Loading saved metadata...\")\n",
    "        class_weights, dataset_info = load_dataset_metadata(metadata_save_path)\n",
    "        return get_saved_data_loaders(data_dir, dataset_info, class_weights, batch_size)\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        transform_val_test = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        print(\"Loading datasets...\")\n",
    "        train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "        val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val_test)\n",
    "        test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform_val_test)\n",
    "        print(\"Datasets loaded successfully.\")\n",
    "\n",
    "        print(\"Calculating class counts...\")\n",
    "        class_counts = [0] * len(emotion_classes)\n",
    "        for _, label in tqdm(train_dataset, desc=\"Counting samples per class\"):\n",
    "            class_counts[label] += 1\n",
    "\n",
    "        total_samples = sum(class_counts)\n",
    "        class_weights = [total_samples / count for count in class_counts]\n",
    "        class_weights = [weight * focus_weights[i] for i, weight in enumerate(class_weights)]\n",
    "        save_dataset_metadata(train_dataset, val_dataset, test_dataset, class_weights, metadata_save_path)\n",
    "\n",
    "        print(\"Creating weighted sampler...\")\n",
    "        sample_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "        print(\"Creating data loaders...\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = prepare_data_loaders(data_dir, batch_size, focus_weights, emotion_classes, metadata_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Model Definition with Dropout\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),  # Increase dropout rate\n",
    "    nn.Linear(model.classifier[1].in_features, num_classes),\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, Optimizer, and Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Added weight decay\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.5619, Val Loss: 1.2202, Val Accuracy: 55.17%\n",
      "Model saved at epoch 1 with Val Accuracy: 55.17%\n",
      "\n",
      "Epoch 2/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 1.3659, Val Loss: 1.0990, Val Accuracy: 59.41%\n",
      "Model saved at epoch 2 with Val Accuracy: 59.41%\n",
      "\n",
      "Epoch 3/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 1.3045, Val Loss: 1.0766, Val Accuracy: 60.46%\n",
      "Model saved at epoch 3 with Val Accuracy: 60.46%\n",
      "\n",
      "Epoch 4/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 1.2626, Val Loss: 1.0580, Val Accuracy: 61.04%\n",
      "Model saved at epoch 4 with Val Accuracy: 61.04%\n",
      "\n",
      "Epoch 5/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 1.2389, Val Loss: 1.0395, Val Accuracy: 61.85%\n",
      "Model saved at epoch 5 with Val Accuracy: 61.85%\n",
      "\n",
      "Epoch 6/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 1.2159, Val Loss: 1.0030, Val Accuracy: 63.76%\n",
      "Model saved at epoch 6 with Val Accuracy: 63.76%\n",
      "\n",
      "Epoch 7/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 1.1996, Val Loss: 1.0191, Val Accuracy: 62.90%\n",
      "\n",
      "Epoch 8/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 1.1755, Val Loss: 0.9990, Val Accuracy: 63.96%\n",
      "Model saved at epoch 8 with Val Accuracy: 63.96%\n",
      "\n",
      "Epoch 9/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 1.1634, Val Loss: 1.0212, Val Accuracy: 63.05%\n",
      "\n",
      "Epoch 10/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 1.1536, Val Loss: 0.9939, Val Accuracy: 64.03%\n",
      "Model saved at epoch 10 with Val Accuracy: 64.03%\n",
      "\n",
      "Epoch 11/20: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 1.1432, Val Loss: 1.0265, Val Accuracy: 63.19%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Training with Early Stopping and Checkpointing\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_save_path, checkpoint_path):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}: Training...\")\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Validating...\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved at epoch {epoch + 1} with Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_save_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_10636\\2528473339.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Progress: 100%|██████████| 413/413 [01:20<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger: 68.18%\n",
      "Disgust: 26.89%\n",
      "Fear: 53.30%\n",
      "Happy: 77.03%\n",
      "Neutral: 67.96%\n",
      "Sad: 64.28%\n",
      "Surprise: 52.60%\n",
      "Overall Test Accuracy: 64.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing with Per-Class Accuracy\n",
    "def test_model(model, test_loader, emotion_classes, model_save_path):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()\n",
    "    correct = [0] * len(emotion_classes)\n",
    "    total = [0] * len(emotion_classes)\n",
    "    print(\"Testing the model...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                total[label] += 1\n",
    "                correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "    for i, emotion in enumerate(emotion_classes):\n",
    "        accuracy = 100 * correct[i] / total[i] if total[i] > 0 else 0\n",
    "        print(f\"{emotion}: {accuracy:.2f}%\")\n",
    "    overall_accuracy = sum(correct) / sum(total)\n",
    "    print(f\"Overall Test Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "\n",
    "test_model(model, test_loader, emotion_classes, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
