{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = \"../DATA_PREPARE_ATT_04/Grayscale_Face_images\"\n",
    "model_save_path = \"optimized_efficientnet_b2_emotion_model.pth\"\n",
    "checkpoint_path = \"optimized_training_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "batch_size = 32\n",
    "accumulation_steps = 4\n",
    "num_epochs = 25\n",
    "initial_lr = 1e-3\n",
    "num_classes = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Emotion categories\n",
    "emotion_classes = [\"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def prepare_data_loaders(data_dir, batch_size):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        transforms.RandomAffine(degrees=0, shear=10, scale=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
    "    ])\n",
    "    transform_val_test = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    with tqdm(total=3, desc=\"Loading datasets\", unit=\"step\") as pbar:\n",
    "        train_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "        pbar.update(1)\n",
    "        val_dataset = datasets.ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val_test)\n",
    "        pbar.update(1)\n",
    "        test_dataset = datasets.ImageFolder(os.path.join(data_dir, \"test\"), transform=transform_val_test)\n",
    "        pbar.update(1)\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "def compute_weights(train_dataset, num_classes):\n",
    "    print(\"Computing class weights...\")\n",
    "    targets = [label for _, label in tqdm(train_dataset, desc=\"Computing class weights\", unit=\"sample\")]\n",
    "    class_weights = compute_class_weight('balanced', classes=np.arange(num_classes), y=targets)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and freeze initial layers\n",
    "def load_model(num_classes, freeze_ratio=0.8):\n",
    "    print(\"Loading and configuring the model...\")\n",
    "    model = models.efficientnet_b2(pretrained=True)\n",
    "    num_layers = len(list(model.parameters()))\n",
    "    layers_to_freeze = int(num_layers * freeze_ratio)\n",
    "\n",
    "    with tqdm(total=num_layers, desc=\"Freezing layers\", unit=\"layer\") as pbar:\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            param.requires_grad = False if i < layers_to_freeze else True\n",
    "            pbar.update(1)\n",
    "\n",
    "    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)  # Adjust for grayscale\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.6),\n",
    "        nn.Linear(model.classifier[1].in_features, num_classes),\n",
    "    )\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, class_weights=None):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.class_weights)(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return ((1 - pt) ** self.gamma * ce_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_save_path, checkpoint_path):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    start_epoch = 0\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}: Training...\")\n",
    "        with tqdm(total=len(train_loader), desc=\"Training Batches\", unit=\"batch\") as pbar:\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                pbar.update(1)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Validating...\")\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation Batches\", unit=\"batch\") as pbar:\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    pbar.update(1)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved at epoch {epoch + 1} with Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets: 100%|██████████| 3/3 [00:00<00:00,  7.86step/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n",
      "Computing class weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing class weights: 100%|██████████| 90043/90043 [20:50<00:00, 72.01sample/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and configuring the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B2_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Freezing layers: 100%|██████████| 301/301 [00:00<00:00, 300663.37layer/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [27:07<00:00,  1.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [04:02<00:00,  2.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 0.3930, Val Loss: 1.4559, Val Accuracy: 24.48%\n",
      "Model saved at epoch 1 with Val Accuracy: 24.48%\n",
      "\n",
      "Epoch 2/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [39:19<00:00,  1.19batch/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [02:34<00:00,  3.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Train Loss: 0.3729, Val Loss: 1.3322, Val Accuracy: 29.49%\n",
      "Model saved at epoch 2 with Val Accuracy: 29.49%\n",
      "\n",
      "Epoch 3/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [33:24<00:00,  1.40batch/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [02:50<00:00,  3.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25, Train Loss: 0.3577, Val Loss: 1.2176, Val Accuracy: 33.52%\n",
      "Model saved at epoch 3 with Val Accuracy: 33.52%\n",
      "\n",
      "Epoch 4/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [23:13<00:00,  2.02batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [03:13<00:00,  3.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25, Train Loss: 0.3442, Val Loss: 1.1037, Val Accuracy: 38.50%\n",
      "Model saved at epoch 4 with Val Accuracy: 38.50%\n",
      "\n",
      "Epoch 5/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [36:47<00:00,  1.27batch/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [03:13<00:00,  3.12batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25, Train Loss: 0.3330, Val Loss: 1.0335, Val Accuracy: 40.58%\n",
      "Model saved at epoch 5 with Val Accuracy: 40.58%\n",
      "\n",
      "Epoch 6/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [38:23<00:00,  1.22batch/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [03:31<00:00,  2.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25, Train Loss: 0.3211, Val Loss: 0.9569, Val Accuracy: 43.19%\n",
      "Model saved at epoch 6 with Val Accuracy: 43.19%\n",
      "\n",
      "Epoch 7/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 2814/2814 [41:43<00:00,  1.12batch/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Batches: 100%|██████████| 603/603 [03:38<00:00,  2.76batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25, Train Loss: 0.3103, Val Loss: 0.9357, Val Accuracy: 44.45%\n",
      "Model saved at epoch 7 with Val Accuracy: 44.45%\n",
      "\n",
      "Epoch 8/25: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:  20%|██        | 572/2814 [05:39<30:41,  1.22batch/s]  "
     ]
    }
   ],
   "source": [
    "# Main Script\n",
    "train_loader, val_loader, test_loader, train_dataset = prepare_data_loaders(data_dir, batch_size)\n",
    "class_weights = compute_weights(train_dataset, num_classes)\n",
    "criterion = WeightedFocalLoss(gamma=2, class_weights=class_weights)\n",
    "model = load_model(num_classes)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr, weight_decay=1e-3)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=initial_lr, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_save_path, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
