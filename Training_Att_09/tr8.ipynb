{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and configuring the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_23576\\1351113480.py:121: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/50:   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_23576\\1351113480.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # ✅ Mixed Precision Training\n",
      "Epoch 1/50: 100%|██████████| 5/5 [00:06<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss: 0.9867, Train Acc: 0.1286 | Val Loss: 1.9407, Val Acc: 0.1500\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 5/5 [00:06<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss: 1.0071, Train Acc: 0.1071 | Val Loss: 1.9424, Val Acc: 0.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss: 0.9697, Train Acc: 0.1857 | Val Loss: 1.9444, Val Acc: 0.1571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss: 0.9936, Train Acc: 0.1143 | Val Loss: 1.9376, Val Acc: 0.1500\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss: 0.9764, Train Acc: 0.1786 | Val Loss: 1.9313, Val Acc: 0.1500\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss: 0.9527, Train Acc: 0.1857 | Val Loss: 1.9214, Val Acc: 0.1857\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 5/5 [00:06<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss: 0.9390, Train Acc: 0.2143 | Val Loss: 1.9120, Val Acc: 0.1857\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss: 0.8950, Train Acc: 0.3786 | Val Loss: 1.9060, Val Acc: 0.1786\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss: 0.8917, Train Acc: 0.3357 | Val Loss: 1.8966, Val Acc: 0.2071\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss: 0.8798, Train Acc: 0.3714 | Val Loss: 1.8898, Val Acc: 0.2143\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss: 0.8289, Train Acc: 0.5000 | Val Loss: 1.8833, Val Acc: 0.2000\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 5/5 [00:06<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Train Loss: 0.8131, Train Acc: 0.4786 | Val Loss: 1.8780, Val Acc: 0.2071\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Train Loss: 0.7528, Train Acc: 0.6143 | Val Loss: 1.8725, Val Acc: 0.1929\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Train Loss: 0.7538, Train Acc: 0.5857 | Val Loss: 1.8666, Val Acc: 0.2286\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 5/5 [00:06<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: Train Loss: 0.6885, Train Acc: 0.6929 | Val Loss: 1.8605, Val Acc: 0.2643\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 5/5 [00:06<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Train Loss: 0.6596, Train Acc: 0.6786 | Val Loss: 1.8510, Val Acc: 0.2929\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Train Loss: 0.6608, Train Acc: 0.6714 | Val Loss: 1.8420, Val Acc: 0.3143\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Train Loss: 0.6323, Train Acc: 0.7000 | Val Loss: 1.8321, Val Acc: 0.3071\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Train Loss: 0.5588, Train Acc: 0.8286 | Val Loss: 1.8178, Val Acc: 0.3143\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Train Loss: 0.5289, Train Acc: 0.7929 | Val Loss: 1.8071, Val Acc: 0.3214\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Train Loss: 0.4862, Train Acc: 0.8429 | Val Loss: 1.7999, Val Acc: 0.3429\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Train Loss: 0.5443, Train Acc: 0.7571 | Val Loss: 1.7929, Val Acc: 0.3286\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Train Loss: 0.4901, Train Acc: 0.7929 | Val Loss: 1.7915, Val Acc: 0.3214\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: Train Loss: 0.4713, Train Acc: 0.8214 | Val Loss: 1.7925, Val Acc: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 5/5 [00:06<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: Train Loss: 0.4183, Train Acc: 0.8857 | Val Loss: 1.7883, Val Acc: 0.2929\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: Train Loss: 0.4408, Train Acc: 0.8357 | Val Loss: 1.7951, Val Acc: 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: Train Loss: 0.3916, Train Acc: 0.8786 | Val Loss: 1.7846, Val Acc: 0.2857\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: Train Loss: 0.3694, Train Acc: 0.8929 | Val Loss: 1.7860, Val Acc: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: Train Loss: 0.3389, Train Acc: 0.9286 | Val Loss: 1.7959, Val Acc: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: Train Loss: 0.3327, Train Acc: 0.9214 | Val Loss: 1.7980, Val Acc: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: Train Loss: 0.3076, Train Acc: 0.9714 | Val Loss: 1.7901, Val Acc: 0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: Train Loss: 0.3178, Train Acc: 0.9357 | Val Loss: 1.7867, Val Acc: 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: Train Loss: 0.3152, Train Acc: 0.9214 | Val Loss: 1.7810, Val Acc: 0.2929\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: Train Loss: 0.3002, Train Acc: 0.9357 | Val Loss: 1.7835, Val Acc: 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: Train Loss: 0.2551, Train Acc: 0.9500 | Val Loss: 1.7832, Val Acc: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: Train Loss: 0.2616, Train Acc: 0.9786 | Val Loss: 1.7870, Val Acc: 0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: Train Loss: 0.2959, Train Acc: 0.8929 | Val Loss: 1.7959, Val Acc: 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 5/5 [00:06<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: Train Loss: 0.2656, Train Acc: 0.9429 | Val Loss: 1.8027, Val Acc: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50:  20%|██        | 1/5 [00:03<00:12,  3.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 188\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Evaluation on Test Set\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, test_loader):\n",
      "Cell \u001b[1;32mIn[2], line 144\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[0;32m    141\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# ✅ Update weights after accumulation steps\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    146\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\anaconda3\\envs\\DeepLearn\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\anaconda3\\envs\\DeepLearn\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\anaconda3\\envs\\DeepLearn\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.efficientnet import EfficientNet_B2_Weights\n",
    "\n",
    "# Paths\n",
    "data_dir = \"../FBMM/test\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"val\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "model_save_path = \"./models/optimized_efficientnet_b2_emotion_model.pth\"\n",
    "\n",
    "# Configuration\n",
    "batch_size = 32  # ✅ Reduced for better generalization\n",
    "num_epochs = 50  # ✅ Increased to 50 with early stopping\n",
    "initial_lr = 1e-3\n",
    "weight_decay = 1e-4  # ✅ L2 Regularization\n",
    "num_classes = 7\n",
    "img_height, img_width = 260, 260\n",
    "seed = 42  # For reproducibility\n",
    "accumulation_steps = 2  # ✅ Gradient accumulation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Emotion categories\n",
    "emotion_classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "# Data Augmentation & Normalization\n",
    "weights = EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    weights.transforms()\n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# Compute Class Weights\n",
    "def compute_class_weights(dataset, num_classes):\n",
    "    labels = np.array([label for _, label in dataset.samples])\n",
    "    class_counts = np.bincount(labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    class_weights /= class_weights.sum()\n",
    "    return torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "class_weights = compute_class_weights(train_dataset, num_classes)\n",
    "\n",
    "# Data Loaders with Optimized Multi-Processing\n",
    "def get_sampler(dataset):\n",
    "    labels = np.array([label for _, label in dataset.samples])\n",
    "    class_sample_counts = np.bincount(labels)\n",
    "    weights = 1.0 / (class_sample_counts[labels] + 1e-6)\n",
    "    return WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, sampler=get_sampler(train_dataset),\n",
    "    num_workers=0, pin_memory=True  # 🚀 Fix for Windows multiprocessing issue\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# Load Pretrained EfficientNet Model & Fine-Tuning\n",
    "def load_model(num_classes):\n",
    "    print(\"Loading and configuring the model...\")\n",
    "    \n",
    "    model = models.efficientnet_b2(weights=weights)  # ✅ Using ImageNet weights\n",
    "    model = model.to(memory_format=torch.channels_last)  # ✅ Optimize memory format for better GPU efficiency\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unlock last 20% of convolutional layers + classifier head\n",
    "    total_layers = len(list(model.features.children()))\n",
    "    fine_tune_layers = int(total_layers * 0.2)\n",
    "\n",
    "    for layer in list(model.features.children())[-fine_tune_layers:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Modify classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.6),\n",
    "        nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    )\n",
    "\n",
    "    # Ensure classifier is trainable\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# Load model\n",
    "model = load_model(num_classes)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr, weight_decay=weight_decay)  # ✅ AdamW with L2 regularization\n",
    "\n",
    "# ✅ Learning Rate Warm-Up & Scheduling\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=initial_lr, epochs=num_epochs, steps_per_epoch=len(train_loader), pct_start=0.1)\n",
    "\n",
    "# ✅ Enable Mixed Precision Training\n",
    "scaler = torch.amp.GradScaler(device=\"cuda\")\n",
    "\n",
    "# Training Loop with Early Stopping & Optimizations\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience=7):\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():  # ✅ Mixed Precision Training\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps  # ✅ Gradient Accumulation\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # ✅ Update weights after accumulation steps\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  # ✅ Apply LR warm-up & decay\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(\"Model Saved!\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "# Evaluation on Test Set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Load best model for testing\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
