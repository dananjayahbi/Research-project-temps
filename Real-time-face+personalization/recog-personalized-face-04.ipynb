{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizes the reference face from the live video (Load reference images from the collected references) (withoput the emotion recognmition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_32700\\1893552541.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion model loaded!\n",
      "âœ… Loaded reference embeddings (Averaged).\n",
      "ðŸš€ Starting real-time face recognition & emotion detection...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… CONFIGURATIONS & LOADING MODELS\n",
    "# --------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# âœ… Load MTCNN for Face Detection (WITH PADDING)\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# âœ… Load Face Recognition Model (FaceNet)\n",
    "facenet = InceptionResnetV1(pretrained=\"casia-webface\").eval().to(device)\n",
    "\n",
    "# âœ… Load Fine-tuned Emotion Detection Model (EfficientNet-B2)\n",
    "emotion_classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "model_save_path = \"efficientnet_b2_emotion_model.pth\"\n",
    "\n",
    "def load_emotion_model(model_path):\n",
    "    print(\"Loading emotion detection model...\")\n",
    "    model = models.efficientnet_b2(pretrained=False)\n",
    "    model.features[0][0] = torch.nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=0.4),  \n",
    "        torch.nn.Linear(model.classifier[1].in_features, len(emotion_classes)),\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"âœ… Emotion model loaded!\")\n",
    "    return model\n",
    "\n",
    "emotion_model = load_emotion_model(model_save_path)\n",
    "\n",
    "# âœ… Reference Face Directory\n",
    "reference_folder = \"./reference-face-frames-collect\"\n",
    "\n",
    "# âœ… Padding for face recognition (NOT for emotion detection)\n",
    "padding_ratio = 0.3  \n",
    "\n",
    "# âœ… Higher similarity threshold for better recognition\n",
    "similarity_threshold = 0.6  \n",
    "\n",
    "# âœ… Quit Button Configuration\n",
    "quit_program = False\n",
    "quit_button_coords = (10, 10, 100, 50)  # (x1, y1, x2, y2)\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… LOAD REFERENCE IMAGES & COMPUTE AVERAGE EMBEDDING\n",
    "# --------------------------------------\n",
    "def get_face_embedding(image):\n",
    "    \"\"\"Detects face and returns its embedding\"\"\"\n",
    "    img_cropped = mtcnn(image)\n",
    "    if img_cropped is None:\n",
    "        return None\n",
    "    if img_cropped.ndim == 3:\n",
    "        img_cropped = img_cropped.unsqueeze(0)\n",
    "    img_cropped = img_cropped.to(device)\n",
    "    embedding = facenet(img_cropped).detach().cpu().numpy().flatten()\n",
    "    return embedding if embedding.shape[0] == 512 else None\n",
    "\n",
    "def load_reference_embeddings(reference_folder):\n",
    "    \"\"\"Loads reference images, extracts embeddings, and returns an averaged embedding\"\"\"\n",
    "    embeddings = []\n",
    "    for file in os.listdir(reference_folder):\n",
    "        if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Consider image files only\n",
    "            image_path = os.path.join(reference_folder, file)\n",
    "            image = Image.open(image_path)\n",
    "            embedding = get_face_embedding(image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "    if len(embeddings) == 0:\n",
    "        return None\n",
    "    \n",
    "    # âœ… Compute the average embedding (for better consistency)\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "# âœ… Load reference embeddings\n",
    "reference_embedding = load_reference_embeddings(reference_folder)\n",
    "\n",
    "if reference_embedding is None:\n",
    "    print(\"âŒ No valid reference faces found! Exiting...\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"âœ… Loaded reference embeddings (Averaged).\")\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… EMOTION DETECTION FUNCTION\n",
    "# --------------------------------------\n",
    "def preprocess_face_for_emotion(face_image):\n",
    "    \"\"\"Preprocesses face for emotion model (WITHOUT PADDING)\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((260, 260)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "    return transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "def predict_emotion(face_image):\n",
    "    \"\"\"Predicts emotion from face image (without padding)\"\"\"\n",
    "    processed_image = preprocess_face_for_emotion(face_image)\n",
    "    with torch.no_grad():\n",
    "        outputs = emotion_model(processed_image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        return emotion_classes[predicted_class_idx], probabilities[0, predicted_class_idx].item()\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… FACE RECOGNITION FUNCTION\n",
    "# --------------------------------------\n",
    "def recognize_face(face_image):\n",
    "    \"\"\"Checks if the detected face matches the reference face\"\"\"\n",
    "    face_embedding = get_face_embedding(Image.fromarray(face_image))\n",
    "    if face_embedding is None:\n",
    "        return False, -1\n",
    "\n",
    "    # âœ… Compute direct cosine similarity\n",
    "    similarity = 1 - cosine(face_embedding, reference_embedding)\n",
    "    \n",
    "    return similarity > similarity_threshold, similarity\n",
    "\n",
    "# âœ… Quit Button Click Handling\n",
    "def check_quit_click(event, x, y, flags, param):\n",
    "    global quit_program\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if quit_button_coords[0] <= x <= quit_button_coords[2] and quit_button_coords[1] <= y <= quit_button_coords[3]:\n",
    "            quit_program = True  # Set flag to exit program\n",
    "\n",
    "# --------------------------------------\n",
    "# âœ… LIVE WEBCAM DETECTION & DISPLAY (FACE + EMOTION)\n",
    "# --------------------------------------\n",
    "def run_live_recognition():\n",
    "    global quit_program\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Error: Cannot access webcam.\")\n",
    "        return\n",
    "\n",
    "    cv2.namedWindow(\"Live Face Recognition & Emotion Detection\")\n",
    "    cv2.setMouseCallback(\"Live Face Recognition & Emotion Detection\", check_quit_click)\n",
    "\n",
    "    print(\"ðŸš€ Starting real-time face recognition & emotion detection...\")\n",
    "\n",
    "    while not quit_program:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"âŒ Error: Cannot read frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, _ = mtcnn.detect(rgb_frame)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "\n",
    "                # âœ… Add padding for better recognition (not for emotion detection)\n",
    "                box_width = x2 - x1\n",
    "                box_height = y2 - y1\n",
    "\n",
    "                pad_w = int(box_width * padding_ratio)\n",
    "                pad_h = int(box_height * padding_ratio)\n",
    "\n",
    "                x1_pad = max(0, x1 - pad_w)\n",
    "                y1_pad = max(0, y1 - pad_h)\n",
    "                x2_pad = min(frame_width, x2 + pad_w)\n",
    "                y2_pad = min(frame_height, y2 + pad_h)\n",
    "\n",
    "                face_image = frame[y1_pad:y2_pad, x1_pad:x2_pad]  # Padded for recognition\n",
    "                cropped_face = frame[y1:y2, x1:x2]  # Exact crop for emotion detection\n",
    "\n",
    "                if cropped_face.shape[0] > 0 and cropped_face.shape[1] > 0:\n",
    "                    is_match, similarity = recognize_face(face_image)\n",
    "\n",
    "                    if is_match:\n",
    "                        emotion, confidence = predict_emotion(cropped_face)\n",
    "                        label = f\"YOU - {emotion} ({confidence:.2f})\"\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "        # Draw Quit Button\n",
    "        cv2.rectangle(frame, quit_button_coords[:2], quit_button_coords[2:], (0, 0, 255), -1)\n",
    "        cv2.putText(frame, \"Quit\", (quit_button_coords[0] + 10, quit_button_coords[1] + 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Live Face Recognition & Emotion Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"ðŸ›‘ Quitting...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# âœ… Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    run_live_recognition()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
