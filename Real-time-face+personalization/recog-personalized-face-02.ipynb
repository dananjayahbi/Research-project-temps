{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizes the reference face from the live video (Load reference images from the folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "e:\\My_GitHub_Repos\\Research-project-temps\\Real-time-face+personalization\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_24356\\2898413732.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Emotion model loaded!\n",
      "✅ Loaded 5 reference embeddings!\n",
      "🎭 Emotion: Neutral (Confidence: 0.94)\n",
      "🎭 Emotion: Neutral (Confidence: 0.98)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.98)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.96)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.98)\n",
      "🎭 Emotion: Neutral (Confidence: 0.97)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 1.00)\n",
      "🎭 Emotion: Neutral (Confidence: 0.98)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 1.00)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 1.00)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 1.00)\n",
      "🎭 Emotion: Neutral (Confidence: 1.00)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.99)\n",
      "🎭 Emotion: Neutral (Confidence: 0.97)\n",
      "🛑 Quitting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ CONFIGURATIONS & LOADING MODELS\n",
    "# --------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load MTCNN for Face Detection\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# ✅ Load Face Recognition Model (FaceNet)\n",
    "facenet = InceptionResnetV1(pretrained=\"casia-webface\").eval().to(device)\n",
    "\n",
    "# ✅ Load Fine-tuned Emotion Detection Model (EfficientNet-B2)\n",
    "emotion_classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "model_save_path = \"efficientnet_b2_emotion_model.pth\"\n",
    "\n",
    "def load_emotion_model(model_path):\n",
    "    print(\"Loading emotion detection model...\")\n",
    "    model = models.efficientnet_b2(pretrained=False)\n",
    "    model.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),  \n",
    "        nn.Linear(model.classifier[1].in_features, len(emotion_classes)),\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"✅ Emotion model loaded!\")\n",
    "    return model\n",
    "\n",
    "emotion_model = load_emotion_model(model_save_path)\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ LOAD ALL REFERENCE FACES FROM FOLDER\n",
    "# --------------------------------------\n",
    "def get_face_embedding(image):\n",
    "    \"\"\"Detects face and returns its embedding\"\"\"\n",
    "    img_cropped = mtcnn(image)\n",
    "    if img_cropped is None:\n",
    "        return None\n",
    "    if img_cropped.ndim == 3:\n",
    "        img_cropped = img_cropped.unsqueeze(0)\n",
    "    img_cropped = img_cropped.to(device)\n",
    "    embedding = facenet(img_cropped).detach().cpu().numpy().flatten()\n",
    "    return embedding if embedding.shape[0] == 512 else None\n",
    "\n",
    "def load_reference_faces(reference_folder):\n",
    "    \"\"\"Loads reference images from a folder and extracts embeddings.\"\"\"\n",
    "    embeddings = []\n",
    "    for file in os.listdir(reference_folder):\n",
    "        if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):  # Consider image files only\n",
    "            image_path = os.path.join(reference_folder, file)\n",
    "            image = Image.open(image_path)\n",
    "            embedding = get_face_embedding(image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "    return embeddings if embeddings else None\n",
    "\n",
    "# ✅ Load reference embeddings from the folder\n",
    "reference_folder = \"./face-resources\"\n",
    "reference_embeddings = load_reference_faces(reference_folder)\n",
    "\n",
    "if reference_embeddings is None:\n",
    "    print(\"❌ No valid reference faces found in the folder! Exiting...\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"✅ Loaded {len(reference_embeddings)} reference embeddings!\")\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ IMAGE PROCESSING FUNCTIONS\n",
    "# --------------------------------------\n",
    "def preprocess_face(face_image):\n",
    "    \"\"\"Preprocesses face for emotion model.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((260, 260)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "    return transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "def predict_emotion(face_image):\n",
    "    \"\"\"Predicts emotion from face image.\"\"\"\n",
    "    processed_image = preprocess_face(face_image)\n",
    "    with torch.no_grad():\n",
    "        outputs = emotion_model(processed_image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        return emotion_classes[predicted_class_idx], probabilities[0, predicted_class_idx].item()\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ LIVE WEBCAM GUI FUNCTION\n",
    "# --------------------------------------\n",
    "quit_program = False\n",
    "quit_button_coords = (10, 10, 100, 50)\n",
    "\n",
    "def check_quit_click(event, x, y, flags, param):\n",
    "    global quit_program\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if quit_button_coords[0] <= x <= quit_button_coords[2] and quit_button_coords[1] <= y <= quit_button_coords[3]:\n",
    "            quit_program = True\n",
    "\n",
    "def recognize_face(face_image):\n",
    "    \"\"\"Checks if the detected face matches the reference face.\"\"\"\n",
    "    face_embedding = get_face_embedding(Image.fromarray(face_image))\n",
    "    if face_embedding is None:\n",
    "        return False, -1\n",
    "    best_similarity = -1\n",
    "    for ref_embedding in reference_embeddings:\n",
    "        similarity = 1 - cosine(face_embedding, ref_embedding)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "    return best_similarity > 0.5, best_similarity\n",
    "\n",
    "# --------------------------------------\n",
    "# ✅ LIVE WEBCAM EMOTION DETECTION (ONLY FOR YOUR FACE)\n",
    "# --------------------------------------\n",
    "def run_live_recognition():\n",
    "    global quit_program\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"❌ Error: Cannot access webcam.\")\n",
    "        return\n",
    "\n",
    "    cv2.namedWindow(\"Face Recognition & Emotion Detection\")\n",
    "    cv2.setMouseCallback(\"Face Recognition & Emotion Detection\", check_quit_click)\n",
    "\n",
    "    while not quit_program:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"❌ Error: Cannot read frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, _ = mtcnn.detect(rgb_frame)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "                face_image = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Ensure face is valid\n",
    "                if face_image.shape[0] > 0 and face_image.shape[1] > 0:\n",
    "                    is_match, similarity = recognize_face(face_image)\n",
    "\n",
    "                    if is_match:\n",
    "                        emotion, confidence = predict_emotion(face_image)\n",
    "                        print(f\"🎭 Emotion: {emotion} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        label = f\"YOU - {emotion} ({confidence:.2f})\"\n",
    "                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    else:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                        cv2.putText(frame, \"Not You\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # Quit button\n",
    "        cv2.rectangle(frame, quit_button_coords[:2], quit_button_coords[2:], (0, 0, 255), -1)\n",
    "        cv2.putText(frame, \"Quit\", (quit_button_coords[0] + 10, quit_button_coords[1] + 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition & Emotion Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or quit_program:\n",
    "            print(\"🛑 Quitting...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ✅ Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    run_live_recognition()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
