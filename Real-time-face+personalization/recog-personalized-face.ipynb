{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizes the reference face from the live video (one by one reference images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tuf\\AppData\\Local\\Temp\\ipykernel_21752\\3519013019.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion model loaded!\n",
      "âœ… Loaded 5 reference embeddings!\n",
      "ðŸŽ­ Emotion: Neutral (Confidence: 0.99)\n",
      "ðŸŽ­ Emotion: Neutral (Confidence: 0.99)\n",
      "ðŸŽ­ Emotion: Neutral (Confidence: 0.99)\n",
      "ðŸŽ­ Emotion: Neutral (Confidence: 0.97)\n",
      "ðŸŽ­ Emotion: Neutral (Confidence: 0.99)\n",
      "ðŸ›‘ Quitting...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "#  CONFIGURATIONS & LOADING MODELS\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#  Load MTCNN for Face Detection\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "#  Load Face Recognition Model (FaceNet)\n",
    "facenet = InceptionResnetV1(pretrained=\"casia-webface\").eval().to(device)\n",
    "\n",
    "#  Load Fine-tuned Emotion Detection Model (EfficientNet-B2)\n",
    "emotion_classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "model_save_path = \"efficientnet_b2_emotion_model.pth\"\n",
    "\n",
    "def load_emotion_model(model_path):\n",
    "    print(\"Loading emotion detection model...\")\n",
    "    model = models.efficientnet_b2(pretrained=False)\n",
    "    model.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.4),  \n",
    "        nn.Linear(model.classifier[1].in_features, len(emotion_classes)),\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\" Emotion model loaded!\")\n",
    "    return model\n",
    "\n",
    "emotion_model = load_emotion_model(model_save_path)\n",
    "\n",
    "\n",
    "#  LOAD REFERENCE FACE (YOUR FACE)\n",
    "\n",
    "def get_face_embedding(image):\n",
    "    img_cropped = mtcnn(image)\n",
    "    if img_cropped is None:\n",
    "        return None\n",
    "    if img_cropped.ndim == 3:\n",
    "        img_cropped = img_cropped.unsqueeze(0)\n",
    "    img_cropped = img_cropped.to(device)\n",
    "    embedding = facenet(img_cropped).detach().cpu().numpy().flatten()\n",
    "    return embedding if embedding.shape[0] == 512 else None\n",
    "\n",
    "def load_reference_face(reference_images):\n",
    "    embeddings = []\n",
    "    for image_path in reference_images:\n",
    "        image = Image.open(image_path)\n",
    "        embedding = get_face_embedding(image)\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings if embeddings else None\n",
    "\n",
    "#  Load YOUR reference images\n",
    "reference_images = [\n",
    "    \"./face-resources/me1.jpg\",\n",
    "    \"./face-resources/me2.jpg\",\n",
    "    \"./face-resources/me3.jpg\",\n",
    "    \"./face-resources/me4.jpg\",\n",
    "    \"./face-resources/me5.jpg\"\n",
    "]\n",
    "reference_embeddings = load_reference_face(reference_images)\n",
    "\n",
    "if reference_embeddings is None:\n",
    "    print(\"No valid reference face found! Exiting...\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\" Loaded {len(reference_embeddings)} reference embeddings!\")\n",
    "\n",
    "\n",
    "#  IMAGE PROCESSING FUNCTIONS\n",
    "\n",
    "def preprocess_face(face_image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((260, 260)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "    return transform(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "def predict_emotion(face_image):\n",
    "    processed_image = preprocess_face(face_image)\n",
    "    with torch.no_grad():\n",
    "        outputs = emotion_model(processed_image)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        return emotion_classes[predicted_class_idx], probabilities[0, predicted_class_idx].item()\n",
    "\n",
    "\n",
    "#  LIVE WEBCAM GUI FUNCTION\n",
    "\n",
    "quit_program = False\n",
    "quit_button_coords = (10, 10, 100, 50)\n",
    "\n",
    "def check_quit_click(event, x, y, flags, param):\n",
    "    global quit_program\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        if quit_button_coords[0] <= x <= quit_button_coords[2] and quit_button_coords[1] <= y <= quit_button_coords[3]:\n",
    "            quit_program = True\n",
    "\n",
    "def recognize_face(face_image):\n",
    "    face_embedding = get_face_embedding(Image.fromarray(face_image))\n",
    "    if face_embedding is None:\n",
    "        return False, -1\n",
    "    best_similarity = -1\n",
    "    for ref_embedding in reference_embeddings:\n",
    "        similarity = 1 - cosine(face_embedding, ref_embedding)\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "    return best_similarity > 0.5, best_similarity\n",
    "\n",
    "\n",
    "#  LIVE WEBCAM EMOTION DETECTION (ONLY FOR YOUR FACE)\n",
    "\n",
    "def run_live_recognition():\n",
    "    global quit_program\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot access webcam.\")\n",
    "        return\n",
    "\n",
    "    cv2.namedWindow(\"Face Recognition & Emotion Detection\")\n",
    "    cv2.setMouseCallback(\"Face Recognition & Emotion Detection\", check_quit_click)\n",
    "\n",
    "    while not quit_program:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read frame from webcam.\")\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, _ = mtcnn.detect(rgb_frame)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "                face_image = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Ensure face is valid\n",
    "                if face_image.shape[0] > 0 and face_image.shape[1] > 0:\n",
    "                    is_match, similarity = recognize_face(face_image)\n",
    "\n",
    "                    if is_match:\n",
    "                        emotion, confidence = predict_emotion(face_image)\n",
    "                        print(f\"Emotion: {emotion} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        label = f\"YOU - {emotion} ({confidence:.2f})\"\n",
    "                        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    else:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                        cv2.putText(frame, \"Not You\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # Quit button\n",
    "        cv2.rectangle(frame, quit_button_coords[:2], quit_button_coords[2:], (0, 0, 255), -1)\n",
    "        cv2.putText(frame, \"Quit\", (quit_button_coords[0] + 10, quit_button_coords[1] + 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Face Recognition & Emotion Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or quit_program:\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "#  Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    run_live_recognition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
