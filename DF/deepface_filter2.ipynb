{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from deepface import DeepFace\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Manually update this path before running each dataset\n",
    "DATASET_FOLDER = r\"C:\\Users\\Tuf\\Downloads\\Compressed\\AffectNet\"  # Change this for each dataset\n",
    "\n",
    "# Define filtered dataset folder (Persistent across runs)\n",
    "FILTERED_DATASET_FOLDER = \"Filtered_Dataset\"\n",
    "\n",
    "# Emotion categories detected by DeepFace\n",
    "EMOTION_CATEGORIES = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the \"Filtered_Dataset\" folder with emotion subfolders\n",
    "def initialize_filtered_dataset():\n",
    "    if not os.path.exists(FILTERED_DATASET_FOLDER):\n",
    "        os.makedirs(FILTERED_DATASET_FOLDER)\n",
    "\n",
    "    for emotion in EMOTION_CATEGORIES:\n",
    "        emotion_folder = os.path.join(FILTERED_DATASET_FOLDER, emotion)\n",
    "        if not os.path.exists(emotion_folder):\n",
    "            os.makedirs(emotion_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Recursively find all image files in the dataset\n",
    "def get_all_images(dataset_path):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Process images and move them into categorized folders\n",
    "def process_images():\n",
    "    image_paths = get_all_images(DATASET_FOLDER)\n",
    "    print(f\"Found {len(image_paths)} images in the dataset: {DATASET_FOLDER}\")\n",
    "\n",
    "    for img_path in tqdm(image_paths, desc=\"Processing Images\"):\n",
    "        try:\n",
    "            # Analyze emotion with enforce_detection=False to prevent errors\n",
    "            result = DeepFace.analyze(img_path=img_path, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "            # Check if a face was detected\n",
    "            if not result or len(result) == 0:\n",
    "                print(f\"Skipping {img_path}: No face detected.\")\n",
    "                continue  # Skip this image\n",
    "\n",
    "            # Extract dominant emotion\n",
    "            dominant_emotion = result[0]['dominant_emotion'].lower()\n",
    "\n",
    "            # Ensure the emotion is one of the known categories\n",
    "            if dominant_emotion in EMOTION_CATEGORIES:\n",
    "                # Define target path\n",
    "                target_folder = os.path.join(FILTERED_DATASET_FOLDER, dominant_emotion)\n",
    "\n",
    "                # Prevent duplicate filenames by appending a unique identifier\n",
    "                unique_name = str(uuid.uuid4())[:8] + \"_\" + os.path.basename(img_path)\n",
    "                target_path = os.path.join(target_folder, unique_name)\n",
    "\n",
    "                # Move image to corresponding folder\n",
    "                shutil.copy(img_path, target_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue  # Skip the current image and proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41553 images in the dataset: C:\\Users\\Tuf\\Downloads\\Compressed\\AffectNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   1%|          | 289/41553 [01:08<2:42:02,  4.24it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      3\u001b[0m     initialize_filtered_dataset()\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing completed for dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can now update DATASET_FOLDER to the next dataset and rerun the script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m, in \u001b[0;36mprocess_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m tqdm(image_paths, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Images\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Analyze emotion with enforce_detection=False to prevent errors\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Check if a face was detected\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\DeepFace.py:253\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\n\u001b[0;32m    167\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    168\u001b[0m     actions: Union[\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     anti_spoofing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    Analyze facial attributes such as age, gender, emotion, and race in the provided image.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m            - 'white': Confidence score for White ethnicity.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdemography\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\modules\\demography.py:123\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# ---------------------------------\u001b[39;00m\n\u001b[0;32m    121\u001b[0m resp_objects \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 123\u001b[0m img_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_obj \u001b[38;5;129;01min\u001b[39;00m img_objs:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m anti_spoofing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m img_obj\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_real\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\modules\\detection.py:95\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, detector_backend, enforce_detection, align, expand_percentage, grayscale, color_face, normalize_face, anti_spoofing)\u001b[0m\n\u001b[0;32m     93\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m [DetectedFace(img\u001b[38;5;241m=\u001b[39mimg, facial_area\u001b[38;5;241m=\u001b[39mbase_region, confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     face_objs \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# in case of no face found\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(face_objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m enforce_detection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\modules\\detection.py:262\u001b[0m, in \u001b[0;36mdetect_faces\u001b[1;34m(detector_backend, img, align, expand_percentage)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# align original image, then find projection of detected face area after alignment\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m align \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:  \u001b[38;5;66;03m# and left_eye is not None and right_eye is not None:\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     aligned_img, angle \u001b[38;5;241m=\u001b[39m \u001b[43malign_img_wrt_eyes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_eye\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_eye\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_eye\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_eye\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     rotated_x1, rotated_y1, rotated_x2, rotated_y2 \u001b[38;5;241m=\u001b[39m project_facial_area(\n\u001b[0;32m    265\u001b[0m         facial_area\u001b[38;5;241m=\u001b[39m(x, y, x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h), angle\u001b[38;5;241m=\u001b[39mangle, size\u001b[38;5;241m=\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    267\u001b[0m     detected_face \u001b[38;5;241m=\u001b[39m aligned_img[\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mint\u001b[39m(rotated_y1) : \u001b[38;5;28mint\u001b[39m(rotated_y2), \u001b[38;5;28mint\u001b[39m(rotated_x1) : \u001b[38;5;28mint\u001b[39m(rotated_x2)\n\u001b[0;32m    269\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\deepface\\modules\\detection.py:314\u001b[0m, in \u001b[0;36malign_img_wrt_eyes\u001b[1;34m(img, left_eye, right_eye)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    313\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mdegrees(np\u001b[38;5;241m.\u001b[39marctan2(left_eye[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m right_eye[\u001b[38;5;241m1\u001b[39m], left_eye[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m right_eye[\u001b[38;5;241m0\u001b[39m])))\n\u001b[1;32m--> 314\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBICUBIC\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, angle\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2350\u001b[0m, in \u001b[0;36mImage.rotate\u001b[1;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[0;32m   2347\u001b[0m     matrix[\u001b[38;5;241m2\u001b[39m], matrix[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m transform(\u001b[38;5;241m-\u001b[39m(nw \u001b[38;5;241m-\u001b[39m w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m-\u001b[39m(nh \u001b[38;5;241m-\u001b[39m h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m, matrix)\n\u001b[0;32m   2348\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m nw, nh\n\u001b[1;32m-> 2350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAFFINE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfillcolor\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2722\u001b[0m, in \u001b[0;36mImage.transform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2718\u001b[0m         im\u001b[38;5;241m.\u001b[39m__transformer(\n\u001b[0;32m   2719\u001b[0m             box, \u001b[38;5;28mself\u001b[39m, Transform\u001b[38;5;241m.\u001b[39mQUAD, quad, resample, fillcolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m         )\n\u001b[0;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2722\u001b[0m     \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\Tuf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2805\u001b[0m, in \u001b[0;36mImage.__transformer\u001b[1;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2803\u001b[0m     resample \u001b[38;5;241m=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    initialize_filtered_dataset()\n",
    "    process_images()\n",
    "    print(f\"Processing completed for dataset: {DATASET_FOLDER}\")\n",
    "    print(\"You can now update DATASET_FOLDER to the next dataset and rerun the script.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
